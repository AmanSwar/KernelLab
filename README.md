
# **KernelLab**  
**High-Performance CUDA Kernels for Deep Learning & HPC**  
Making GPUs go Brrrrr....
![LOGO](https://github.com/AmanSwar/KernelLab/blob/master/logo.png)
![CUDA](https://img.shields.io/badge/CUDA-Optimized-green?style=for-the-badge&logo=nvidia)  
![C++](https://img.shields.io/badge/C%2B%2B-17%2B-blue?style=for-the-badge&logo=c%2B%2B)  

## **About KernelLab**  
**KernelLab** is a collection of **highly optimized CUDA kernels** designed for deep learning, high-performance computing (HPC), and general-purpose GPU acceleration. Each kernel includes multiple levels of optimization‚Äîfrom **na√Øve implementations** to **shared memory, warp-level, vectorized, and tensor-core optimized** versions.  

---

## **üõ†Ô∏è Features**  
- **Optimized CUDA kernels** for **deep learning, matrix operations, and image processing**  
- **Multiple optimization techniques**: Shared Memory, Coalesced Memory Access, Warp-Level Parallelism, Tensor Cores  
- **Benchmark comparisons against cuBLAS, cuDNN, and PyTorch CUDA kernels**  
- **Currently Optimized for Ampere architecture (cuz I am GPU poor)**  

---

## **üìå Implemented Kernels & Optimizations**  

### **üîπ Convolution Kernels**  
| Kernel  | Optimization Levels |  
|---------|--------------------|  
| **2D Convolution (Conv2D)** | 1Ô∏è‚É£ **Na√Øve (Direct element-wise computation)** <br> 2Ô∏è‚É£ **Tiled Shared Memory (Minimizing global memory access)** <br> 3Ô∏è‚É£ **Memory Coalescing (Optimized memory access patterns)** <br> 4Ô∏è‚É£ **Tensor Cores (Using WMMA for fused matrix multiplications)** |  
| **3D Convolution (Conv3D)** | 1Ô∏è‚É£ **Na√Øve** <br> 2Ô∏è‚É£ **Shared Memory (Minimizing redundant loads)** <br> 3Ô∏è‚É£ **Tiled (Reducing register pressure)** <br> 4Ô∏è‚É£ **Register Blocking (Efficient memory reuse via registers)** |  

### **üîπ Matrix & Reduction Operations**  
| Kernel  | Optimization Levels |  
|---------|--------------------|  
| **Matrix Transpose** | 1Ô∏è‚É£ **Na√Øve (Direct row-column swaps)** <br> 2Ô∏è‚É£ **Shared Memory Tiling (Blocking memory accesses for fewer global loads)** <br> 3Ô∏è‚É£ **Memory Coalescing (Optimizing global memory writes for aligned access)** |  
| **Matrix Multiplication (GEMM)** | 1Ô∏è‚É£ **Na√Øve (Row-major computation)** <br> 2Ô∏è‚É£ **Tiled (Using shared memory for efficient blocking)** <br> 3Ô∏è‚É£ **Register Blocking (Reducing register pressure & maximizing reuse)** <br> 4Ô∏è‚É£ **Warp-Level Tiling (Optimizing warp-level data exchange)** <br> 5Ô∏è‚É£ **Tensor Cores with WMMA (Using NVIDIA Tensor Cores for fused matrix ops)** |  
| **Reduction Sum** | 1Ô∏è‚É£ **Na√Øve (Basic sequential reduction per thread block)** <br> 2Ô∏è‚É£ **Branchless Reduction (Avoiding thread divergence for performance gain)** <br> 3Ô∏è‚É£ **Warp-Level Reduction (Using shuffle intrinsics for direct register exchange)** |  

### **üîπ Element-wise & Activation Functions**  
| Kernel  | Optimization Levels |  
|---------|--------------------|  
| **ReLU Activation** | 1Ô∏è‚É£ **Na√Øve (Basic element-wise ReLU application)** <br> 2Ô∏è‚É£ **Coalesced Memory Access (Optimized read/write for better bandwidth usage)** <br> 3Ô∏è‚É£ **Vectorized Execution (Processing multiple elements per thread using vector types like `float4`)** |  
| **SoftMax Function** | 1Ô∏è‚É£ **Na√Øve (Computing exponentials & normalizing sequentially)** <br> 2Ô∏è‚É£ **Shared Memory Optimization (Avoiding redundant memory accesses)** <br> 3Ô∏è‚É£ **Block Tiling (Parallelizing exponentiation & normalization)** <br> 4Ô∏è‚É£ **Warp-Level Reduction (Efficient sum-reduction across warps)** <br> 5Ô∏è‚É£ **State-of-the-Art Optimization (Optimized numerical stability & memory efficiency)** |  
| **Vector Addition** | 1Ô∏è‚É£ **Na√Øve (Thread-per-element)** <br> 2Ô∏è‚É£ **Shared Memory Optimization (Minimizing redundant memory loads)** <br> 3Ô∏è‚É£ **Tiled Execution (Using block-level parallelism for efficiency)** <br> 4Ô∏è‚É£ **Coalesced Memory Access (Optimizing memory loads for aligned access)** <br> 5Ô∏è‚É£ **Vectorized Computation (Using `float4` for processing multiple elements per thread)** <br> 6Ô∏è‚É£ **Multi-Element Processing (Reducing loop overhead for large arrays)** |  

### **üîπ Image Processing Kernels**  
| Kernel  | Optimization Levels |  
|---------|--------------------|  
| **Greyscale Conversion** | 1Ô∏è‚É£ **Na√Øve (Direct pixel-wise computation)** <br> 2Ô∏è‚É£ **Shared Memory Optimization (Reducing redundant loads per thread block)** <br> 3Ô∏è‚É£ **Memory Coalescing (Ensuring aligned memory accesses for better bandwidth)** <br> 4Ô∏è‚É£ **Vectorized Computation (`uchar4` processing per thread)** <br> 5Ô∏è‚É£ **Multi-Pixel Processing (Parallel processing of multiple pixels per thread)** <br> 6Ô∏è‚É£ **Fused Multiply-Add (FMA) Optimization (Combining operations for fewer instructions)** |  
| **Image Blurring** | 1Ô∏è‚É£ **Na√Øve (Basic kernel filter computation per pixel)** <br> 2Ô∏è‚É£ **Optimized Shared Memory Tiling (Minimizing global memory accesses by loading tiles into shared memory)** |  

---

## **üìù Currenlty implementing / TODO & Future Plans**  
- [ ] **Self-Attention CUDA Kernel**  
- [ ] **Flash Attention Kernel Optimization**  
- [ ] **LeakyReLU Kernel**  
- [ ] **Layer Normalization CUDA Kernel**  
- [ ] **FFT, BFS, DFS, and Sorting CUDA Implementations**  

---

## **üìú License**  
KernelLab is **open-source** under the **MIT License**.  

---

## **ü§ù Contributing**  
PRs and issues are welcome! If you have an optimization idea or find a bug, feel free to contribute. üöÄ  

---

### **üí¨ Contact**  
For discussions & suggestions, open an issue or DM me on GitHub!  

---

### **üî• KernelLab: Pushing CUDA Performance to the Next Level! üî•**  

---
